import pandas as pd
import numpy as np
import re
import os
from tqdm import tqdm
import warnings

# ì‹œê°í™” ë¼ì´ë¸ŒëŸ¬ë¦¬ ì¶”ê°€
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
# í°íŠ¸ ê´€ë¦¬ ëª¨ë“ˆ ì¶”ê°€
from matplotlib import font_manager, rc

# ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ ì„¤ì •
warnings.filterwarnings("ignore")

# ê°ì„± ë¶„ì„ ë° NLP ë¼ì´ë¸ŒëŸ¬ë¦¬
from textblob import TextBlob
from collections import Counter
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk import pos_tag

from sklearn.feature_extraction.text import TfidfVectorizer

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ (í•„ìš”í•œ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ìë™ ë‹¤ìš´ë¡œë“œ ì‹œë„)
nltk.download("averaged_perceptron_tagger")
nltk.download("averaged_perceptron_tagger_eng")
try:
    nltk.data.find("tokenizers/punkt")
except LookupError:
    nltk.download("punkt")
try:
    nltk.data.find("corpora/stopwords")
except LookupError:
    nltk.download("stopwords")


# ============================================================================
# âš ï¸ [ìµœì¢… ìˆ˜ì •] í•œê¸€ í°íŠ¸ ì„¤ì • (Mac OS ìµœì í™” ë° ì˜¤ë¥˜ ë°©ì§€)
# ============================================================================
WORDCLOUD_FONT_PATH = '/System/Library/Fonts/Supplemental/AppleGothic.ttf'

try:
    # matplotlibì˜ í°íŠ¸ ë§¤ë‹ˆì €ë¥¼ ì´ìš©í•´ í°íŠ¸ ì´ë¦„ ì„¤ì •
    font_name = font_manager.FontProperties(fname=WORDCLOUD_FONT_PATH).get_name()
    rc('font', family=font_name, size=10) # í°íŠ¸ í¬ê¸° ì¡°ì •
    print(f"   âœ“ ì‹œê°í™” í°íŠ¸ ì„¤ì • ì™„ë£Œ: {font_name}")
except Exception:
    # í°íŠ¸ ê²½ë¡œ ì˜¤ë¥˜ ì‹œ fallback ì„¤ì •
    rc('font', family='AppleGothic', size=10)
    print("   âš ï¸ ê²½ê³ : ê¸°ë³¸ í°íŠ¸ ì„¤ì •ìœ¼ë¡œ ëŒ€ì²´ë˜ì—ˆìŠµë‹ˆë‹¤.")

plt.rcParams['axes.unicode_minus'] = False # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€


# ============================================================================
print("=" * 80)
print("ğŸ“¦ ì¿ íŒ¡ ë¦¬ë·° ê°ì„± ë¶„ì„ ì‹œìŠ¤í…œ (origianl_coupang.csv ê¸°ì¤€)")
print("=" * 80)


# ë¦¬ë·° íŒŒì¼ëª… ì„¤ì •
COUPANG_REVIEWS_FILE = "origianl_coupang.csv"
OUTPUT_FOLDER = "Coupang_review_analysis"


# ============================================================================
# 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# ============================================================================

def clean_text(text):
    """í…ìŠ¤íŠ¸ë¥¼ ì •ì œí•©ë‹ˆë‹¤. (í•œê¸€ì„ ìœ ì§€í•˜ë„ë¡ ìˆ˜ì •)"""
    if pd.isna(text):
        return ""
    text = str(text).lower()
    text = re.sub(r"[^a-zA-Z0-9\sê°€-í£]", " ", text) 
    text = re.sub(r"\s+", " ", text)
    return text.strip()


def categorize_sentiment(rating):
    """í‰ì ì„ ê¸°ì¤€ìœ¼ë¡œ ê°ì„±ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
    if rating >= 4:
        return "positive"
    elif rating >= 3:
        return "neutral"
    else:
        return "negative"


def load_and_preprocess_data(reviews_path):
    """ë¦¬ë·° ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    print("\n[1/5] ë°ì´í„° ë¡œë”© ì¤‘...")

    reviews_df = pd.read_csv(reviews_path, encoding="utf-8-sig")
    reviews_df["review_text_clean"] = reviews_df["review_text"].apply(clean_text)
    reviews_df["sentiment_category"] = reviews_df["rating"].apply(categorize_sentiment)

    print(f"   âœ“ ì´ {len(reviews_df)}ê°œ ë¦¬ë·° ë¡œë“œ ì™„ë£Œ")
    return reviews_df


# ============================================================================
# 2. ê°ì„± ë¶„ì„ (TextBlob ì‚¬ìš©)
# ============================================================================

def perform_sentiment_analysis(df):
    """TextBlobì„ ì‚¬ìš©í•œ ìƒì„¸ ê°ì„± ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    print("\n[2/5] ê°ì„± ë¶„ì„ ìˆ˜í–‰ ì¤‘...")
    sentiments = []
    subjectivities = []
    
    # í…ìŠ¤íŠ¸ê°€ í•œêµ­ì–´ì´ë¯€ë¡œ TextBlobì˜ ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šê³ , 
    # í‰ì  ê¸°ë°˜ ê°ì„± ë¶„ë¥˜ë¥¼ ë©”ì¸ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. 
    # (TextBlobì€ ì˜ì–´ ê¸°ë°˜ìœ¼ë¡œ Polarityë§Œ ì°¸ê³ )
    for text in tqdm(df["review_text"], desc="   ê°ì„± ì ìˆ˜ ê³„ì‚°"): 
        blob = TextBlob(str(text))
        sentiments.append(blob.sentiment.polarity)
        subjectivities.append(blob.sentiment.subjectivity)
    df["sentiment_polarity"] = sentiments
    df["sentiment_subjectivity"] = subjectivities
    print(f"   âœ“ ê°ì„± ë¶„ì„ ì™„ë£Œ")
    return df


# ============================================================================
# 3. í‚¤ì›Œë“œ ì¶”ì¶œ (TF-IDF)
# ============================================================================

def get_extended_stopwords():
    """ê¸°ë³¸ ì˜ë¬¸ ë¶ˆìš©ì–´ì™€ ì œí’ˆ ê´€ë ¨ ë¶ˆìš©ì–´ë¥¼ ê²°í•©í•©ë‹ˆë‹¤."""
    base_stopwords = set(stopwords.words("english"))
    custom_stopwords = {
        "probiotic", "product", "supplement", "capsule", "day", "take", 
        "love", "like", "feel", "use", "time", "one", "try", "also",
        "would", "get", "bought", "really", "much", "since", "well", "go",
        "make", "start", "see", "back", "first", "even", "still", "thing",
        "find", "lot", "way", "last", "came", "ordered", "hope", "put",
        "think", "getting", "know", "sure", "need", "used", "though",
        "good", "great", "best",
    }
    return base_stopwords.union(custom_stopwords)


def is_meaningful_keyword(word):
    """ë‹¨ì–´ì˜ ê¸¸ì´ì™€ í’ˆì‚¬ íƒœê·¸ë¥¼ ê²€ì‚¬í•˜ì—¬ ì˜ë¯¸ìˆëŠ” í‚¤ì›Œë“œì¸ì§€ íŒë‹¨í•©ë‹ˆë‹¤."""
    if len(word) < 3 or word.isdigit():
        return False
    
    try:
        tag = pos_tag([word])[0][1]
        return tag.startswith("NN") or tag.startswith("JJ")
    except:
        return True


def filter_meaningful_keywords(keywords_with_scores, min_word_length=3):
    """ì¶”ì¶œëœ í‚¤ì›Œë“œ ëª©ë¡ì—ì„œ ì˜ë¯¸ì—†ëŠ” ë‹¨ì–´ë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤. (í•œê¸€ì€ í†µê³¼)"""
    return [
        (w, s) for w, s in keywords_with_scores 
        if is_meaningful_keyword(w) or re.search(r'[ê°€-í£]', w)
    ]


def extract_keywords_by_sentiment(df, n_keywords=15):
    """TF-IDFë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì„±ë³„ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    print("\n[3/5] í‚¤ì›Œë“œ ì¶”ì¶œ ì¤‘...")
    extended_stopwords = get_extended_stopwords()
    keywords_by_sentiment = {}
    
    for sentiment in ["positive", "neutral", "negative"]:
        texts = df[df["sentiment_category"] == sentiment]["review_text_clean"].tolist()
        
        if not texts or all(not text.strip() for text in texts):
            keywords_by_sentiment[sentiment] = []
            continue

        vectorizer = TfidfVectorizer(
            max_features=300,
            stop_words=list(extended_stopwords),
            ngram_range=(1, 2),
            min_df=1,
            max_df=0.9,
        )
        
        try:
            tfidf_matrix = vectorizer.fit_transform(texts)
            
            if tfidf_matrix.shape[1] == 0:
                 keywords_by_sentiment[sentiment] = []
                 continue
                 
            feature_names = vectorizer.get_feature_names_out()
            mean_tfidf = np.asarray(tfidf_matrix.mean(axis=0)).flatten()
            
            top_indices = mean_tfidf.argsort()[-50:][::-1]
            candidate_keywords = [(feature_names[i], mean_tfidf[i]) for i in top_indices]
            
            filtered_keywords = filter_meaningful_keywords(candidate_keywords)
            
            keywords_by_sentiment[sentiment] = filtered_keywords[:n_keywords]
        
        except ValueError:
            keywords_by_sentiment[sentiment] = []
            
    print("   âœ“ í‚¤ì›Œë“œ ì¶”ì¶œ ì™„ë£Œ")
    return keywords_by_sentiment

# ============================================================================
# 4. ê³ ê° ë¶ˆë§Œ ì˜ì—­ ìë™ ë„ì¶œ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
# ============================================================================

def extract_improvement_insights(df):
    """ì‚¬ì „ ì •ì˜ëœ í‚¤ì›Œë“œ ë”•ì…”ë„ˆë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë¶ˆë§Œ ì˜ì—­ì„ ë¶„ë¥˜í•˜ê³  ìš°ì„ ìˆœìœ„ë¥¼ ë„ì¶œí•©ë‹ˆë‹¤."""
    print("\n[4/5] ê°œì„  í¬ì¸íŠ¸ ë¶„ì„ ì¤‘...")
    
    concern_reviews = df[df["sentiment_category"].isin(["neutral", "negative"])].copy()
    if concern_reviews.empty:
        print("   âœ“ 0ê°œ ê°œì„  ì˜ì—­ ë°œê²¬")
        return []

    improvement_areas = {
        "side_effects": {"keywords": ["bloat", "bloating", "gas", "nausea", "headache", "stomach", "pain"], "mentions": [], "description": "ë¶€ì‘ìš©/ë¶ˆí¸í•¨"},
        "packaging_quality": {"keywords": ["package", "bottle", "seal", "broken", "expire", "smell"], "mentions": [], "description": "í¬ì¥/í’ˆì§ˆ ë¬¸ì œ"},
        "effectiveness": {"keywords": ["ineffective", "not work", "no effect", "no difference"], "mentions": [], "description": "íš¨ê³¼/íš¨ëŠ¥ ë¶€ì¡±"},
        "price_value": {"keywords": ["expensive", "cost", "pricey"], "mentions": [], "description": "ê°€ê²©/ê°€ì¹˜ ë¶ˆë§Œì¡±"},
        "swallowing": {"keywords": ["swallow", "big", "large", "taste"], "mentions": [], "description": "ì„­ì·¨ì˜ ì–´ë ¤ì›€/ë§›"},
        "delivery": {"keywords": ["delivery", "late", "shipping"], "mentions": [], "description": "ë°°ì†¡ ë¶ˆë§Œ"},
    }

    for index, row in concern_reviews.iterrows():
        review_text = row["review_text_clean"]
        
        for area_code, data in improvement_areas.items():
            found_keywords = [k for k in data["keywords"] if k in review_text]
            
            if found_keywords:
                data["mentions"].append({
                    "rating": row["rating"],
                    "text": row["review_text"],
                    "keywords": found_keywords
                })

    prioritized_areas = []
    for code, data in improvement_areas.items():
        if data["mentions"]:
            prioritized_areas.append({
                "code": code,
                "description": data["description"],
                "count": len(data["mentions"]),
                "samples": data["mentions"],
            })
            
    prioritized_areas.sort(key=lambda x: x["count"], reverse=True)

    print(f"   âœ“ {len(prioritized_areas)}ê°œ ê°œì„  ì˜ì—­ ë°œê²¬")
    return prioritized_areas


# ============================================================================
# 5. ì‹œê°í™” ë° ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„± (4-in-1 ì°¨íŠ¸ ë¡œì§ ë³µì› ë° ê°•í™”)
# ============================================================================

def create_4in1_sentiment_chart(df, keywords_with_scores, sentiment, output_folder):
    """ì›í•˜ì‹œëŠ” ëŒ€ë¡œ 4ê°€ì§€ ì°¨íŠ¸(í‰ì , ê°ì„±, í‚¤ì›Œë“œ ë°”, ì›Œë“œí´ë¼ìš°ë“œ)ë¥¼ 
    í•˜ë‚˜ì˜ ì´ë¯¸ì§€ íŒŒì¼ì— ìƒì„±í•©ë‹ˆë‹¤."""
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12)) 
    fig.suptitle(f'{sentiment.capitalize()} ë¦¬ë·° ì¢…í•© ë¶„ì„ ëŒ€ì‹œë³´ë“œ', fontsize=20, y=1.02)
    
    # ---------------- 1. í‰ì  ë¶„í¬ (Rating Distribution) ----------------
    sns.countplot(x='rating', data=df[df['sentiment_category'] == sentiment], ax=axes[0, 0], palette='viridis')
    axes[0, 0].set_title('1. í‰ì  ë¶„í¬', fontsize=14)
    axes[0, 0].set_xlabel('í‰ì  (Rating)', fontsize=12)
    axes[0, 0].set_ylabel('ë¦¬ë·° ìˆ˜', fontsize=12)
    
    # ---------------- 2. ê°ì„± ê·¹ì„± ë¶„í¬ (Sentiment Polarity Distribution) ----------------
    sns.histplot(df[df['sentiment_category'] == sentiment]['sentiment_polarity'], bins=10, kde=True, ax=axes[0, 1], color='coral')
    axes[0, 1].set_title('2. ê°ì„± ê·¹ì„± ë¶„í¬', fontsize=14)
    axes[0, 1].set_xlabel('ê°ì„± ê·¹ì„± (Polarity) -1.0 ~ 1.0', fontsize=12)
    axes[0, 1].set_ylabel('ë¹ˆë„', fontsize=12)
    
    # ---------------- 3. ìƒìœ„ í‚¤ì›Œë“œ ë°” ì°¨íŠ¸ (Top Keyword Bar Chart) ----------------
    top_keywords = pd.DataFrame(keywords_with_scores, columns=['Keyword', 'Score'])
    if not top_keywords.empty:
        top_keywords = top_keywords.sort_values(by='Score', ascending=True).tail(10)
        sns.barplot(x='Score', y='Keyword', data=top_keywords, ax=axes[1, 0], color='skyblue')
        axes[1, 0].set_title('3. ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ (TF-IDF)', fontsize=14)
        axes[1, 0].set_xlabel('TF-IDF ì ìˆ˜', fontsize=12)
        axes[1, 0].set_ylabel('í‚¤ì›Œë“œ', fontsize=12)
    else:
        axes[1, 0].text(0.5, 0.5, 'í‚¤ì›Œë“œ ì—†ìŒ', ha='center', va='center', fontsize=16)
        axes[1, 0].set_title('3. ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ (TF-IDF)', fontsize=14)
        axes[1, 0].axis('off')

    # ---------------- 4. ì›Œë“œ í´ë¼ìš°ë“œ (Word Cloud) ----------------
    keyword_dict = {k: v for k, v in keywords_with_scores}
    if keyword_dict:
        wc = WordCloud(
            font_path=WORDCLOUD_FONT_PATH,
            width=500, # ì„œë¸Œí”Œë¡¯ í¬ê¸°ì— ë§ì¶¤
            height=300, # ì„œë¸Œí”Œë¡¯ í¬ê¸°ì— ë§ì¶¤
            background_color="white",
            max_words=30,
            normalize_plurals=False
        )
        wc.generate_from_frequencies(keyword_dict)
        axes[1, 1].imshow(wc, interpolation="bilinear")
        axes[1, 1].set_title('4. ì£¼ìš” í‚¤ì›Œë“œ ì›Œë“œ í´ë¼ìš°ë“œ', fontsize=14)
        axes[1, 1].axis("off")
    else:
        axes[1, 1].text(0.5, 0.5, 'í‚¤ì›Œë“œ ì—†ìŒ', ha='center', va='center', fontsize=16)
        axes[1, 1].set_title('4. ì£¼ìš” í‚¤ì›Œë“œ ì›Œë“œ í´ë¼ìš°ë“œ', fontsize=14)
        axes[1, 1].axis('off')

    # âš ï¸ ìµœì¢… ì €ì¥ ì§ì „: ì €ì¥ ì˜¤ë¥˜ ë°©ì§€ ë° ë ˆì´ì•„ì›ƒ ìµœì í™”
    plt.tight_layout(rect=[0, 0, 1, 0.98])
    plt.savefig(
        os.path.join(output_folder, f"analysis_{sentiment}.png"),
        dpi=150,           # ê³ í•´ìƒë„ ì§€ì •
        transparent=False  # íˆ¬ëª… ë°°ê²½ ì œê±° (ì €ì¥ ì˜¤ë¥˜ ë°©ì§€)
    )
    plt.close()


def create_visualizations(df, keywords_by_sentiment, improvement_areas):
    """ëª¨ë“  ì‹œê°í™” íŒŒì¼ì„ ìƒì„±í•˜ê³  ì €ì¥í•©ë‹ˆë‹¤."""
    print("\n[5/5] ì‹œê°í™” ìƒì„± ì¤‘...")
    output_folder = OUTPUT_FOLDER
    
    # 1. ì „ì²´ ê°ì„± ë¶„í¬ ìš”ì•½ ì°¨íŠ¸ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    # create_summary_chart(df, output_folder) # ì´ ì°¨íŠ¸ëŠ” 4-in-1ì— í¬í•¨ë˜ì§€ ì•Šì•„ ìƒëµ

    # 2. ê°ì„±ë³„ ì¢…í•© ì°¨íŠ¸ (4-in-1 ëŒ€ì‹œë³´ë“œ)
    for sentiment, keywords in keywords_by_sentiment.items():
        # ì¤‘ë¦½ ë¦¬ë·°ì˜ ê²½ìš° ë°ì´í„°ê°€ ì ì–´ ì˜¤ë¥˜ ê°€ëŠ¥ì„±ì´ ìˆì–´ ì˜ˆì™¸ ì²˜ë¦¬
        if sentiment == 'neutral' and len(df[df['sentiment_category'] == sentiment]) < 10:
             print(f"   âš ï¸ {sentiment} ë¦¬ë·° ìˆ˜ê°€ ë¶€ì¡±í•˜ì—¬ ì¢…í•© ëŒ€ì‹œë³´ë“œ ìƒëµ")
             continue

        create_4in1_sentiment_chart(df, keywords, sentiment, output_folder)
        print(f"   âœ“ {sentiment} ì¢…í•© ëŒ€ì‹œë³´ë“œ ì €ì¥: {os.path.join(output_folder, f'analysis_{sentiment}.png')}")
            
    print(f"   âœ“ ëª¨ë“  ì‹œê°í™” ì™„ë£Œ")


def generate_analysis_report(df, keywords_by_sentiment, improvement_areas):
    # (ë¦¬í¬íŠ¸ ìƒì„± ë¡œì§ì€ ë™ì¼í•˜ë¯€ë¡œ ìƒëµ)
    # ... ì´ì „ ì½”ë“œì™€ ë™ì¼í•˜ê²Œ ìœ ì§€ ...
    print("\n[ì¶”ê°€] ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    output_folder = OUTPUT_FOLDER
    report_path = os.path.join(output_folder, "analysis_report.txt")
    
    total_reviews = len(df)
    avg_rating = df['rating'].mean()
    sentiment_counts = df['sentiment_category'].value_counts(normalize=True)
    pos_ratio = sentiment_counts.get('positive', 0)
    neu_ratio = sentiment_counts.get('neutral', 0)
    neg_ratio = sentiment_counts.get('negative', 0)
    
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("=" * 80 + "\n")
        f.write("ì¿ íŒ¡ ë¦¬ë·° ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸\n")
        f.write("=" * 80 + "\n\n")

        # 1. ì „ì²´ í†µê³„
        f.write("[ 1. ì „ì²´ í†µê³„ ]\n")
        f.write("-" * 80 + "\n")
        f.write(f"ì´ ë¦¬ë·° ìˆ˜: {total_reviews:,}ê°œ\n")
        f.write(f"í‰ê·  í‰ì : {avg_rating:.2f} / 5.0\n")
        f.write(f"ê¸ì • ë¦¬ë·° ë¹„ìœ¨: {pos_ratio:.1%}\n")
        f.write(f"ì¤‘ë¦½ ë¦¬ë·° ë¹„ìœ¨: {neu_ratio:.1%}\n")
        f.write(f"ë¶€ì • ë¦¬ë·° ë¹„ìœ¨: {neg_ratio:.1%}\n")
        f.write(f"í‰ê·  ê°ì„± ê·¹ì„±: {df['sentiment_polarity'].mean():.3f} (-1 ~ 1)\n\n")

        # 2. ê°ì„±ë³„ ì£¼ìš” í‚¤ì›Œë“œ
        f.write("[ 2. ê°ì„±ë³„ ì£¼ìš” í‚¤ì›Œë“œ ]\n")
        f.write("-" * 80 + "\n")
        for sentiment, keywords in keywords_by_sentiment.items():
            f.write(f"\nâ–  {sentiment.upper()} ë¦¬ë·° í‚¤ì›Œë“œ:\n")
            if keywords:
                for i, (word, score) in enumerate(keywords):
                    f.write(f"   {i+1:2d}. {word:30} (TF-IDF: {score:.4f})\n")
            else:
                f.write("   (í‚¤ì›Œë“œ ì—†ìŒ)\n")

        # 3. ê³ ê° ë¶ˆë§Œ ì˜ì—­ ë¶„ì„
        f.write("\n[ 3. ê³ ê° ë¶ˆë§Œ ì˜ì—­ ë¶„ì„ (ì¤‘ë¦½/ë¶€ì • ë¦¬ë·° ê¸°ì¤€) ]\n")
        f.write("-" * 80 + "\n")
        if not improvement_areas:
            f.write("ë¶„ì„ëœ ì£¼ìš” ë¶ˆë§Œ ì˜ì—­ì´ ì—†ìŠµë‹ˆë‹¤. (í•œêµ­ì–´ ë¦¬ë·°ë¡œ ì¸í•œ ì˜ë¬¸ í‚¤ì›Œë“œ ë¯¸ë§¤ì¹­ ê°€ëŠ¥ì„±)\n")
        else:
            for i, area in enumerate(improvement_areas):
                f.write(f"\n[{i+1}ìˆœìœ„] {area['description']}\n")
                f.write(f"  â”” ì–¸ê¸‰ íšŸìˆ˜: {area['count']}íšŒ\n")
                f.write(f"  â”” ì˜ì—­ ì½”ë“œ: {area['code']}\n")
                f.write(f"  â”” ê³ ê° ì˜ê²¬ ìƒ˜í”Œ:\n")
                for j, sample in enumerate(area['samples'][:2]):
                    f.write(f"     {j+1}. (í‰ì  {sample['rating']:.1f}) {sample['text'][:100]}...\n") 

    print(f"   âœ“ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
    return report_path


# ============================================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ìµœì¢…)
# ============================================================================

if __name__ == "__main__":
    if not os.path.exists(COUPANG_REVIEWS_FILE):
        print(f"âŒ ì˜¤ë¥˜: {COUPANG_REVIEWS_FILE} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   íŒŒì¼ëª…ì„ í™•ì¸í•˜ê±°ë‚˜, ìŠ¤í¬ë¦½íŠ¸ì™€ ê°™ì€ í´ë”ì— íŒŒì¼ì„ ë‘ì„¸ìš”.")
    else:
        try:
            reviews_df = load_and_preprocess_data(COUPANG_REVIEWS_FILE)
            reviews_df = perform_sentiment_analysis(reviews_df)
            keywords_by_sentiment = extract_keywords_by_sentiment(reviews_df, n_keywords=15)
            improvement_areas = extract_improvement_insights(reviews_df)
            
            if not os.path.exists(OUTPUT_FOLDER):
                os.makedirs(OUTPUT_FOLDER)
                
            # 4-in-1 ì‹œê°í™” ìƒì„±
            create_visualizations(reviews_df, keywords_by_sentiment, improvement_areas)
            
            # ë¦¬í¬íŠ¸ ìƒì„±
            report_path = generate_analysis_report(reviews_df, keywords_by_sentiment, improvement_areas)
            
            print("\n" + "=" * 80)
            print("âœ… ì¿ íŒ¡ ë¦¬ë·° ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! (4-in-1 ëŒ€ì‹œë³´ë“œ í˜•ì‹)")
            print("=" * 80)
            print(f"ê²°ê³¼ë¬¼ì€ '{OUTPUT_FOLDER}' í´ë”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

        except Exception as e:
            print(f"\nâŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            import traceback
            traceback.print_exc()