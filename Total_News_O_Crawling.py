import csv
import time
from datetime import datetime, timedelta
import re
import os

# ì›¹ ë¸Œë¼ìš°ì € ì œì–´ ë° HTML ë¶„ì„
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup

# êµ¬ê¸€ ë‰´ìŠ¤ ê²€ìƒ‰, AI ë¶„ì„, ë¬¸ì¥ ë¶„ë¦¬
from pygooglenews import GoogleNews
# from transformers import pipeline # ê°ì„± ë¶„ì„ ë¹„í™œì„±í™”
# import kss # ê°ì„± ë¶„ì„ ë¹„í™œì„±í™”
import feedparser

# --- ì„¤ì • ---
# ğŸ’¡ [ìˆ˜ì •ë¨] 9ê°œì˜ í‚¤ì›Œë“œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ê´€ë¦¬
KEYWORD_LIST = [
    "ë¹„ì—”ë‚ ì”¬ ë‹¤ì´ì–´íŠ¸ ìœ ì‚°ê· ", "ë¹„ì—”ë‚ ì”¬ ë‹¤ì´ì–´íŠ¸", "ë¹„ì—”ë‚ ì”¬",
    "ë‹¤ì´ì–´íŠ¸ ìœ ì‚°ê· ", "ë‹¤ì´ì–´íŠ¸", "ìœ ì‚°ê· ",
    "ë¹„ì—”ë‚ ì”¬ ìœ ì‚°ê· ", "ìœ ì‚°ê·  ë‹¤ì´ì–´íŠ¸", "ë‹¤ì´ì–´íŠ¸ ë¹„ì—”ë‚ ì”¬"
]
CSV_FILENAME = "í†µí•©ë‰´ìŠ¤_ë¶„ì„_ëŒ€ëŸ‰ìˆ˜ì§‘.csv"
# ------------------------------------

def parse_date(date_str):
    now = datetime.now()
    if "ë¶„ ì „" in date_str:
        mins = int(re.search(r'\d+', date_str).group())
        return now - timedelta(minutes=mins)
    elif "ì‹œê°„ ì „" in date_str:
        hours = int(re.search(r'\d+', date_str).group())
        return now - timedelta(hours=hours)
    elif "ì–´ì œ" in date_str:
        return now - timedelta(days=1)
    try: 
        return datetime.strptime(date_str.replace('.', ''), '%Y%m%d')
    except ValueError:
        return now

def search_major_rss_feeds(keyword, limit=50): # ë” ë§ì€ ê²°ê³¼ë¥¼ ìœ„í•´ limit ìƒí–¥
    """ì£¼ìš” ì–¸ë¡ ì‚¬ RSS í”¼ë“œì—ì„œ í‚¤ì›Œë“œ ê´€ë ¨ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    # ... (ì´ì „ê³¼ ë™ì¼) ...
    print(f"  - ì–¸ë¡ ì‚¬ RSS í”¼ë“œì—ì„œ '{keyword}' ê²€ìƒ‰ ì¤‘...")
    rss_feeds = {
        'ì¡°ì„ ì¼ë³´': 'https://www.chosun.com/arc/outboundfeeds/rss/?outputType=xml', 'ì¤‘ì•™ì¼ë³´': 'https://rss.joins.com/joins_news_list.xml',
        'ë™ì•„ì¼ë³´': 'https://rss.donga.com/total.xml', 'í•œê²¨ë ˆ': 'https://www.hani.co.kr/rss/',
        'ê²½í–¥ì‹ ë¬¸': 'https://www.khan.co.kr/rss/', 'YTN': 'https://www.ytn.co.kr/rss/ytn_news_major.xml'
    }
    all_articles = []
    for press, url in rss_feeds.items():
        try:
            feed = feedparser.parse(url)
            for entry in feed.entries:
                if keyword.split(' ')[0] in entry.title or (hasattr(entry, 'summary') and keyword.split(' ')[0] in entry.summary):
                    published_time = datetime.fromtimestamp(time.mktime(entry.published_parsed))
                    all_articles.append({'title': entry.title, 'link': entry.link, 'source': press, 'published': published_time, 'source_portal': 'ì–¸ë¡ ì‚¬RSS'})
        except Exception: continue
    all_articles.sort(key=lambda x: x['published'], reverse=True)
    return all_articles[:limit]


def search_google_news(keyword, limit=50): # ë” ë§ì€ ê²°ê³¼ë¥¼ ìœ„í•´ limit ìƒí–¥
    """êµ¬ê¸€ ë‰´ìŠ¤ì—ì„œ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤."""
    print(f"  - êµ¬ê¸€ ë‰´ìŠ¤ì—ì„œ '{keyword}' ê²€ìƒ‰ ì¤‘...")
    gn = GoogleNews(lang='ko', country='KR')
    search_result = gn.search(keyword, when='30d') # ê²€ìƒ‰ ê¸°ê°„ì„ 30ì¼ë¡œ í™•ëŒ€
    articles = []
    for item in search_result['entries'][:limit]:
        articles.append({'title': item.title, 'link': item.link, 'source': item.source['title'],
                         'published': datetime.strptime(item.published, '%a, %d %b %Y %H:%M:%S %Z'),
                         'source_portal': 'êµ¬ê¸€ë‰´ìŠ¤'})
    return articles


def news_collection_process(keyword, writer, last_sequence, existing_links):
    """í•˜ë‚˜ì˜ í‚¤ì›Œë“œì— ëŒ€í•´ ë‰´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê³  íŒŒì¼ì— ì”ë‹ˆë‹¤."""
    
    all_articles = []
    all_articles.extend(search_google_news(keyword))
    all_articles.extend(search_major_rss_feeds(keyword))
    all_articles.sort(key=lambda x: x['published'], reverse=True)
    
    unique_articles = []
    for article in all_articles:
        if article['link'] not in existing_links:
            unique_articles.append(article)
    
    if not unique_articles:
        print(f"  -> '{keyword}'ì— ëŒ€í•œ ìƒˆë¡œìš´ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return 0 # ìƒˆë¡œ ì¶”ê°€ëœ ê°œìˆ˜ 0

    new_items_count = 0
    for i, item in enumerate(unique_articles):
        current_sequence = last_sequence + i + 1
        
        news_data = {
            "ìˆœë²ˆ": current_sequence,
            "ê¸°ë¡ì¼ì‹œ": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "ë‚ ì§œ": item['published'].strftime('%Y-%m-%d %H:%M'),
            "ê²€ìƒ‰í‚¤ì›Œë“œ": keyword,
            "ì¶œì²˜í¬í„¸": item['source_portal'],
            "ì¶œì²˜": item['source'],
            "ì œëª©": item['title'],
            "ê¸°ì‚¬í‰ê°€": "ë¶„ì„ ì œì™¸",
            "ê¸°ì‚¬í‰ê°€ê·¼ê±°ìš”ì•½": "ë¶„ì„ ì œì™¸",
            "ë§í¬": item['link']
        }
        writer.writerow(news_data)
        existing_links.add(item['link']) # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ ë§í¬ ì¶”ê°€
        new_items_count += 1

    print(f"  -> '{keyword}' ê²€ìƒ‰ ê²°ê³¼, {new_items_count}ê°œì˜ ìƒˆë¡œìš´ ë‰´ìŠ¤ë¥¼ ì €ì¥í–ˆìŠµë‹ˆë‹¤.")
    return new_items_count

if __name__ == "__main__":
    
    file_exists = os.path.isfile(CSV_FILENAME)
    last_sequence = 0
    existing_links = set()
    
    if file_exists:
        print("ê¸°ì¡´ ë°ì´í„°ë¥¼ ì½ì–´ì˜µë‹ˆë‹¤...")
        with open(CSV_FILENAME, 'r', newline='', encoding='utf-8-sig') as f:
            reader = csv.reader(f)
            header = next(reader, None)
            if header:
                try:
                    link_index = header.index('ë§í¬')
                    for row in reader:
                        if row: 
                            last_sequence = int(row[0])
                            existing_links.add(row[link_index])
                except (ValueError, IndexError): pass
        print(f"ì´ {last_sequence}ê°œì˜ ê¸°ì¡´ ë°ì´í„°ê°€ ìˆìŠµë‹ˆë‹¤. ì´ì–´ì„œ ì €ì¥í•©ë‹ˆë‹¤.")

    # ğŸ’¡ [ìˆ˜ì •ë¨] íŒŒì¼ì„ í•œ ë²ˆë§Œ ì—´ê³  ëª¨ë“  í‚¤ì›Œë“œì— ëŒ€í•´ ì‘ì—… ìˆ˜í–‰
    with open(CSV_FILENAME, "a", newline="", encoding="utf-8-sig") as csvfile:
        fieldnames = ["ìˆœë²ˆ", "ê¸°ë¡ì¼ì‹œ", "ë‚ ì§œ", "ê²€ìƒ‰í‚¤ì›Œë“œ", "ì¶œì²˜í¬í„¸", "ì¶œì²˜", "ì œëª©", "ê¸°ì‚¬í‰ê°€", "ê¸°ì‚¬í‰ê°€ê·¼ê±°ìš”ì•½", "ë§í¬"]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        
        if not file_exists or os.path.getsize(CSV_FILENAME) == 0:
            writer.writeheader()
        
        total_new_count = 0
        # ğŸ’¡ [ìˆ˜ì •ë¨] í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒí•˜ë©° ì‘ì—… ìˆ˜í–‰
        for keyword in KEYWORD_LIST:
            print("\n" + "â”€" * 70)
            print(f"í‚¤ì›Œë“œ '{keyword}' ê²€ìƒ‰ ì‹œì‘...")
            added_count = news_collection_process(keyword, writer, last_sequence, existing_links)
            total_new_count += added_count
            last_sequence += added_count # ë‹¤ìŒ í‚¤ì›Œë“œë¥¼ ìœ„í•´ ë§ˆì§€ë§‰ ìˆœë²ˆ ì—…ë°ì´íŠ¸
            time.sleep(2) # ë‹¤ìŒ í‚¤ì›Œë“œ ê²€ìƒ‰ ì „ ì ì‹œ ëŒ€ê¸°

    print("â”€" * 70); print(f"\nâœ… ëª¨ë“  í‚¤ì›Œë“œ ê²€ìƒ‰ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ì´ {total_new_count}ê°œì˜ ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.")