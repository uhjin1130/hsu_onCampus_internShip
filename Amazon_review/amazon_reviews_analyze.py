import pandas as pd
import numpy as np
import re
import os
from tqdm import tqdm
import warnings

warnings.filterwarnings("ignore")

# ê°ì„± ë¶„ì„ ë° NLP
from textblob import TextBlob
from collections import Counter
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk import pos_tag

# ì‹œê°í™”
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

from sklearn.feature_extraction.text import TfidfVectorizer

# NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ
nltk.download("averaged_perceptron_tagger")
nltk.download("averaged_perceptron_tagger_eng")
try:
    nltk.data.find("tokenizers/punkt")
except LookupError:
    nltk.download("punkt")
try:
    nltk.data.find("corpora/stopwords")
except LookupError:
    nltk.download("stopwords")

# ============================================================================
print("=" * 80)
print("ë‹¤ì´ì–´íŠ¸ ìœ ì‚°ê·  ë¦¬ë·° ë¶„ì„ ì‹œìŠ¤í…œ")
print("=" * 80)


# ============================================================================
# 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
# ============================================================================


def load_and_preprocess_data(reviews_path, products_path):
    """ë¦¬ë·° ë° ì œí’ˆ ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤."""
    print("\n[1/5] ë°ì´í„° ë¡œë”© ì¤‘...")

    reviews_df = pd.read_csv(reviews_path, encoding="utf-8-sig")
    products_df = pd.read_csv(products_path, encoding="utf-8-sig")

    # ë¦¬ë·° í…ìŠ¤íŠ¸ ì •ì œ
    reviews_df["review_text_clean"] = reviews_df["review_text"].apply(clean_text)

    # ê°ì„± ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ (í‰ì  ê¸°ì¤€)
    reviews_df["sentiment_category"] = reviews_df["rating"].apply(categorize_sentiment)

    print(f"   âœ“ ì´ {len(reviews_df)}ê°œ ë¦¬ë·° ë¡œë“œ ì™„ë£Œ")
    print(f"   âœ“ ì´ {len(products_df)}ê°œ ì œí’ˆ ì •ë³´ ë¡œë“œ ì™„ë£Œ")

    return reviews_df, products_df


def clean_text(text):
    """í…ìŠ¤íŠ¸ë¥¼ ì •ì œí•©ë‹ˆë‹¤."""
    if pd.isna(text):
        return ""
    text = text.lower()
    text = re.sub(r"[^a-zA-Z0-9\s]", " ", text)
    text = re.sub(r"\s+", " ", text)
    return text.strip()


def categorize_sentiment(rating):
    """í‰ì ì„ ê¸°ì¤€ìœ¼ë¡œ ê°ì„±ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤."""
    if rating >= 4:
        return "positive"
    elif rating >= 3:
        return "neutral"
    else:
        return "negative"


# ============================================================================
# 2. ê°ì„± ë¶„ì„
# ============================================================================


def perform_sentiment_analysis(df):
    """TextBlobì„ ì‚¬ìš©í•œ ìƒì„¸ ê°ì„± ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    print("\n[2/5] ê°ì„± ë¶„ì„ ìˆ˜í–‰ ì¤‘...")

    sentiments = []
    subjectivities = []
    for text in tqdm(df["review_text_clean"], desc="   ê°ì„± ì ìˆ˜ ê³„ì‚°"):
        blob = TextBlob(text)
        sentiments.append(blob.sentiment.polarity)
        subjectivities.append(blob.sentiment.subjectivity)
    df["sentiment_polarity"] = sentiments
    df["sentiment_subjectivity"] = subjectivities
    print(f"   âœ“ ê°ì„± ë¶„ì„ ì™„ë£Œ")
    return df


# ============================================================================
# 3. ê°•í™”ëœ ë¶ˆìš©ì–´+ëª…ì‚¬/í˜•ìš©ì‚¬ í•„í„° ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
# ============================================================================


def get_extended_stopwords():
    base_stopwords = set(stopwords.words("english"))
    custom_stopwords = {
        # ê¸°ì¡´ ë° ê°œì„  ìš”ì²­ ë°˜ì˜
        "probiotic",
        "probiotics",
        "product",
        "products",
        "supplement",
        "supplements",
        "capsule",
        "capsules",
        "pill",
        "pills",
        "tablet",
        "tablets",
        "gummy",
        "gummies",
        "bottle",
        "bottles",
        "box",
        "boxes",
        "package",
        "packages",
        "container",
        "day",
        "days",
        "week",
        "weeks",
        "month",
        "months",
        "year",
        "years",
        "time",
        "times",
        "first",
        "second",
        "third",
        "one",
        "two",
        "three",
        "30",
        "60",
        "90",
        "100",
        "120",
        "daily",
        "every",
        "morning",
        "night",
        "evening",
        "buy",
        "bought",
        "purchase",
        "purchased",
        "order",
        "ordered",
        "ordering",
        "amazon",
        "seller",
        "delivery",
        "shipping",
        "arrived",
        "receive",
        "received",
        "take",
        "taking",
        "took",
        "taken",
        "use",
        "using",
        "used",
        "get",
        "getting",
        "got",
        "try",
        "trying",
        "tried",
        "start",
        "started",
        "starting",
        "begin",
        "began",
        "continue",
        "continued",
        "stop",
        "stopped",
        "make",
        "making",
        "made",
        "go",
        "going",
        "went",
        "gone",
        "come",
        "coming",
        "came",
        "good",
        "great",
        "best",
        "better",
        "nice",
        "fine",
        "ok",
        "okay",
        "bad",
        "worse",
        "worst",
        "poor",
        "terrible",
        "horrible",
        "really",
        "very",
        "much",
        "more",
        "most",
        "less",
        "least",
        "just",
        "only",
        "also",
        "even",
        "still",
        "yet",
        "always",
        "never",
        "sometimes",
        "recommend",
        "recommended",
        "recommendation",
        "review",
        "reviews",
        "rating",
        "star",
        "stars",
        "love",
        "like",
        "liked",
        "dislike",
        "work",
        "works",
        "worked",
        "working",
        "help",
        "helps",
        "helped",
        "helping",
        "effect",
        "effects",
        "result",
        "results",
        "notice",
        "noticed",
        "noticing",
        "see",
        "seeing",
        "saw",
        "seen",
        "feel",
        "feeling",
        "felt",
        "seem",
        "seems",
        "seemed",
        "appear",
        "appears",
        "appeared",
        "thing",
        "things",
        "something",
        "anything",
        "everything",
        "nothing",
        "want",
        "wanted",
        "need",
        "needed",
        "may",
        "might",
        "could",
        "would",
        "should",
        "think",
        "thought",
        "know",
        "knew",
        "sure",
        "however",
        "although",
        "though",
        # ì¶”ê°€ ê°•í™”: ì•„ë˜ëŠ” ê°œì„  ìš”ì²­ ë° ë„ë©”ì¸ë³„ ì˜ë¯¸ ì—†ìŒ â†’ price, swallow ì œì™¸!
        "well",
        "easy",
        "difference",
        "back",
        "since",
        "bit",
        "long",
        "life",
        "said",
        "full",
        "brand",
        "dissolve",
        "different",
        "body",
        "looked",
        "definitely",
        "regular",
        "keep",
        "issues",
        "fiber",
        "highly",
        "ones",
        "new",
        "way",
        "little",
        "twice",
        "people",
        "change",
        "read",
        "chew",
        "collagen",
        "benefits",
        "open",
        "hard",
        "size",
        "large",
    }
    # price, swallowëŠ” ì œê±°!
    return base_stopwords.union(custom_stopwords - {"price", "swallow"})


def is_meaningful_keyword(word):
    # ìµœì†Œ 3ì, ìˆ«ìì œì™¸, ì¤‘ë³µë¬¸ìì œì™¸, ì˜ì–´ ì•ŒíŒŒë²³
    if len(word) < 3:
        return False
    if word.isdigit():
        return False
    if any(char.isdigit() for char in word):
        return False
    if len(set(word)) == 1:
        return False
    # ëª…ì‚¬/í˜•ìš©ì‚¬ í•„í„° ì ìš© (POS tagging)
    tag = pos_tag([word])[0][1]
    if not (tag.startswith("NN") or tag.startswith("JJ")):
        return False
    return True


def filter_meaningful_keywords(keywords_with_scores, min_word_length=3):
    return [(w, s) for w, s in keywords_with_scores if is_meaningful_keyword(w)]


def extract_keywords_by_sentiment(df, n_keywords=15):
    extended_stopwords = get_extended_stopwords()
    keywords_by_sentiment = {}
    for sentiment in ["positive", "neutral", "negative"]:
        texts = df[df["sentiment_category"] == sentiment]["review_text_clean"].tolist()
        if not texts:
            keywords_by_sentiment[sentiment] = []
            continue
        vectorizer = TfidfVectorizer(
            max_features=300,
            stop_words=list(extended_stopwords),
            ngram_range=(1, 2),
            min_df=3,
            max_df=0.5,
        )
        tfidf_matrix = vectorizer.fit_transform(texts)
        feature_names = vectorizer.get_feature_names_out()
        mean_tfidf = np.asarray(tfidf_matrix.mean(axis=0)).flatten()
        top_indices = mean_tfidf.argsort()[-50:][::-1]
        candidate_keywords = [(feature_names[i], mean_tfidf[i]) for i in top_indices]
        filtered_keywords = filter_meaningful_keywords(candidate_keywords)
        keywords_by_sentiment[sentiment] = filtered_keywords[:n_keywords]
    return keywords_by_sentiment


# ============================================================================
# 4. ê³ ê° ë¶ˆë§Œ ì˜ì—­ ìë™ ë„ì¶œ(ê¸°ì¡´ ìœ ì§€)
# ============================================================================


def extract_improvement_insights(df):
    print("\n[4/5] ê°œì„  í¬ì¸íŠ¸ ë¶„ì„ ì¤‘...")
    concern_reviews = df[df["sentiment_category"].isin(["negative", "neutral"])]
    if len(concern_reviews) == 0:
        return {}
    improvement_areas = {
        "effectiveness": {
            "keywords": [
                "ineffective",
                "not work",
                "no effect",
                "no result",
                "no change",
                "didnt work",
                "doesnt work",
                "useless",
                "waste",
                "disappointed",
                "nothing",
                "zero",
                "fake",
            ],
            "mentions": [],
            "description": "íš¨ê³¼/íš¨ëŠ¥ ë¶€ì¡±",
        },
        "side_effects": {
            "keywords": [
                "bloat",
                "bloating",
                "gas",
                "gassy",
                "upset stomach",
                "nausea",
                "cramping",
                "cramps",
                "diarrhea",
                "constipation",
                "uncomfortable",
                "pain",
                "ache",
                "sick",
                "headache",
                "allergy",
                "allergic",
                "rash",
            ],
            "mentions": [],
            "description": "ë¶€ì‘ìš©/ë¶ˆí¸í•¨",
        },
        "taste_smell": {
            "keywords": [
                "taste",
                "aftertaste",
                "smell",
                "odor",
                "stink",
                "fishy",
                "bitter",
                "sour",
                "disgusting",
                "nasty",
                "unpleasant",
                "flavor",
            ],
            "mentions": [],
            "description": "ë§›/ëƒ„ìƒˆ ë¬¸ì œ",
        },
        "size_difficulty": {
            "keywords": [
                "large",
                "huge",
                "big",
                "swallow",
                "choke",
                "hard swallow",
                "difficult swallow",
                "horse pill",
                "size",
                "gigantic",
            ],
            "mentions": [],
            "description": "í¬ê¸°/ë³µìš© ë¶ˆí¸",
        },
        "price_value": {
            "keywords": [
                "expensive",
                "overpriced",
                "costly",
                "pricey",
                "too much money",
                "not worth",
                "poor value",
                "waste money",
                "cheap quality",
            ],
            "mentions": [],
            "description": "ê°€ê²©/ê°€ì¹˜ ë¶ˆë§Œì¡±",
        },
        "packaging_quality": {
            "keywords": [
                "packaging",
                "broken",
                "damaged",
                "leak",
                "leaking",
                "seal",
                "unsealed",
                "melted",
                "stuck together",
                "quality control",
                "expire",
            ],
            "mentions": [],
            "description": "í¬ì¥/í’ˆì§ˆ ë¬¸ì œ",
        },
        "ingredient_concerns": {
            "keywords": [
                "artificial",
                "sugar",
                "sweetener",
                "additive",
                "filler",
                "chemical",
                "synthetic",
                "dye",
                "color",
                "preservative",
                "gmo",
            ],
            "mentions": [],
            "description": "ì„±ë¶„ ìš°ë ¤",
        },
        "dosage_inconvenience": {
            "keywords": [
                "multiple times",
                "too many",
                "several times",
                "inconvenient",
                "forget",
                "refrigerate",
                "refrigeration",
                "storage",
            ],
            "mentions": [],
            "description": "ë³µìš©ëŸ‰/ë³´ê´€ ë¶ˆí¸",
        },
    }
    for idx, row in concern_reviews.iterrows():
        review_text = row["review_text_clean"]
        for area, info in improvement_areas.items():
            for keyword in info["keywords"]:
                if keyword in review_text:
                    improvement_areas[area]["mentions"].append(
                        {
                            "review_id": row["review_id"],
                            "product_name": row["product_name"],
                            "rating": row["rating"],
                            "keyword": keyword,
                            "review_snippet": review_text[:200],
                        }
                    )
                    break
    prioritized_areas = []
    for area, info in improvement_areas.items():
        mention_count = len(info["mentions"])
        if mention_count > 0:
            prioritized_areas.append(
                {
                    "area": area,
                    "description": info["description"],
                    "mention_count": mention_count,
                    "sample_reviews": info["mentions"][:3],
                }
            )
    prioritized_areas.sort(key=lambda x: x["mention_count"], reverse=True)
    print(f"   âœ“ {len(prioritized_areas)}ê°œ ê°œì„  ì˜ì—­ ë°œê²¬")
    return prioritized_areas


# ============================================================================
# 5. ì‹œê°í™” ë° ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±(ì œí’ˆë³„ í‰ì  ì œê±°)
# ============================================================================


def create_visualizations(df, keywords_by_sentiment, improvement_areas):
    print("\n[5/5] ì‹œê°í™” ìƒì„± ì¤‘...")
    output_folder = "Amazon_review_analysis"
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    plt.rcParams['font.family'] = 'Malgun Gothic'  # Windows ë§‘ì€ ê³ ë”•
    plt.rcParams['axes.unicode_minus'] = False
    for sentiment in ["positive", "neutral", "negative"]:
        sentiment_df = df[df["sentiment_category"] == sentiment]
        if len(sentiment_df) == 0:
            continue
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle(
            f"Review Analysis - {sentiment.upper()}", fontsize=20, fontweight="bold"
        )
        # 1. í‰ì  ë¶„í¬
        ax1 = axes[0, 0]
        rating_counts = sentiment_df["rating"].value_counts().sort_index()
        ax1.bar(
            rating_counts.index,
            rating_counts.values,
            color=get_sentiment_color(sentiment),
            alpha=0.7,
            edgecolor="black",
        )
        ax1.set_title(
            f"Rating Distribution ({sentiment})", fontsize=14, fontweight="bold"
        )
        ax1.set_xlabel("Rating", fontsize=12)
        ax1.set_ylabel("Count", fontsize=12)
        ax1.grid(axis="y", alpha=0.3)
        # 2. ê°ì„± ê·¹ì„± ë¶„í¬
        ax2 = axes[0, 1]
        ax2.hist(
            sentiment_df["sentiment_polarity"],
            bins=30,
            color=get_sentiment_color(sentiment),
            alpha=0.7,
            edgecolor="black",
        )
        ax2.set_title(
            f"Sentiment Polarity Distribution ({sentiment})",
            fontsize=14,
            fontweight="bold",
        )
        ax2.set_xlabel("Polarity Score", fontsize=12)
        ax2.set_ylabel("Frequency", fontsize=12)
        ax2.axvline(
            sentiment_df["sentiment_polarity"].mean(),
            color="red",
            linestyle="--",
            linewidth=2,
            label="Mean",
        )
        ax2.legend()
        ax2.grid(axis="y", alpha=0.3)
        # 3. ì£¼ìš” í‚¤ì›Œë“œ
        ax3 = axes[1, 0]
        if (
            sentiment in keywords_by_sentiment
            and len(keywords_by_sentiment[sentiment]) > 0
        ):
            keywords = keywords_by_sentiment[sentiment][:12]
            words = [kw[0] for kw in keywords]
            scores = [kw[1] for kw in keywords]
            y_pos = np.arange(len(words))
            ax3.barh(y_pos, scores, color=get_sentiment_color(sentiment), alpha=0.7)
            ax3.set_yticks(y_pos)
            ax3.set_yticklabels(words, fontsize=10)
            ax3.invert_yaxis()
            ax3.set_title(f"Top Keywords ({sentiment})", fontsize=14, fontweight="bold")
            ax3.set_xlabel("TF-IDF Score", fontsize=12)
            ax3.grid(axis="x", alpha=0.3)
        # 4. ì›Œë“œ í´ë¼ìš°ë“œ
        ax4 = axes[1, 1]
        text_data = " ".join(sentiment_df["review_text_clean"].tolist())
        if len(text_data) > 50:
            extended_stopwords = get_extended_stopwords()
            wordcloud = WordCloud(
                width=800,
                height=400,
                background_color="white",
                stopwords=extended_stopwords,
                colormap=get_wordcloud_colormap(sentiment),
                max_words=80,
                relative_scaling=0.5,
                min_font_size=10,
            ).generate(text_data)
            ax4.imshow(wordcloud, interpolation="bilinear")
            ax4.axis("off")
            ax4.set_title(f"Word Cloud ({sentiment})", fontsize=14, fontweight="bold")
        plt.tight_layout()
        output_path = os.path.join(output_folder, f"analysis_{sentiment}.png")
        plt.savefig(output_path, dpi=300, bbox_inches="tight")
        plt.close()
        print(f"   âœ“ {sentiment} ì‹œê°í™” ì €ì¥: {output_path}")

    # ì „ì²´ ìš”ì•½ ì°¨íŠ¸
    create_summary_chart(df, improvement_areas, output_folder)
    print(f"   âœ“ ëª¨ë“  ì‹œê°í™” ì™„ë£Œ")


def get_sentiment_color(sentiment):
    colors = {"positive": "#2ecc71", "neutral": "#f39c12", "negative": "#e74c3c"}
    return colors.get(sentiment, "#95a5a6")


def get_wordcloud_colormap(sentiment):
    colormaps = {"positive": "Greens", "neutral": "Oranges", "negative": "Reds"}
    return colormaps.get(sentiment, "Blues")


def create_summary_chart(df, improvement_areas, output_folder):
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    fig.suptitle("Overall Analysis Summary", fontsize=20, fontweight="bold")
    ax1 = axes[0, 0]
    sentiment_counts = df["sentiment_category"].value_counts()
    colors = [get_sentiment_color(s) for s in sentiment_counts.index]
    wedges, texts, autotexts = ax1.pie(
        sentiment_counts.values,
        labels=sentiment_counts.index,
        autopct="%1.1f%%",
        colors=colors,
        startangle=90,
    )
    for autotext in autotexts:
        autotext.set_color("white")
        autotext.set_fontsize(12)
        autotext.set_fontweight("bold")
    ax1.set_title("Sentiment Distribution", fontsize=14, fontweight="bold")
    ax2 = axes[0, 1]
    rating_counts = df["rating"].value_counts().sort_index()
    bars = ax2.bar(
        rating_counts.index,
        rating_counts.values,
        color="#3498db",
        alpha=0.7,
        edgecolor="black",
    )
    ax2.set_title("Rating Distribution", fontsize=14, fontweight="bold")
    ax2.set_xlabel("Rating", fontsize=12)
    ax2.set_ylabel("Count", fontsize=12)
    ax2.grid(axis="y", alpha=0.3)
    for bar in bars:
        height = bar.get_height()
        ax2.text(
            bar.get_x() + bar.get_width() / 2.0,
            height,
            f"{int(height)}",
            ha="center",
            va="bottom",
            fontsize=10,
        )
    ax3 = axes[1, 0]
    if improvement_areas and len(improvement_areas) > 0:
        top_areas = improvement_areas[:8]
        categories = [area["description"] for area in top_areas]
        counts = [area["mention_count"] for area in top_areas]
        y_pos = np.arange(len(categories))
        bars = ax3.barh(y_pos, counts, color="#e74c3c", alpha=0.7, edgecolor="black")
        ax3.set_yticks(y_pos)
        ax3.set_yticklabels(categories, fontsize=10)
        ax3.invert_yaxis()
        ax3.set_title(
            "Improvement Areas (Negative/Neutral Reviews)",
            fontsize=14,
            fontweight="bold",
        )
        ax3.set_xlabel("Mention Count", fontsize=11)
        ax3.grid(axis="x", alpha=0.3)
        for i, bar in enumerate(bars):
            width = bar.get_width()
            ax3.text(
                width + 0.5,
                bar.get_y() + bar.get_height() / 2.0,
                f"{int(width)}",
                ha="left",
                va="center",
                fontsize=9,
                fontweight="bold",
            )
    else:
        ax3.text(
            0.5,
            0.5,
            "No significant issues found",
            ha="center",
            va="center",
            fontsize=12,
            transform=ax3.transAxes,
        )
        ax3.set_xlim(0, 1)
        ax3.set_ylim(0, 1)
        ax3.axis("off")
    ax4 = axes[1, 1]
    sentiment_data = []
    sentiment_labels = []
    for sentiment in ["positive", "neutral", "negative"]:
        data = df[df["sentiment_category"] == sentiment]["sentiment_polarity"].values
        if len(data) > 0:
            sentiment_data.append(data)
            sentiment_labels.append(sentiment)
    if sentiment_data:
        bp = ax4.boxplot(sentiment_data, labels=sentiment_labels, patch_artist=True)
        for patch, sentiment in zip(bp["boxes"], sentiment_labels):
            patch.set_facecolor(get_sentiment_color(sentiment))
            patch.set_alpha(0.7)
        ax4.set_title("Sentiment Polarity by Category", fontsize=14, fontweight="bold")
        ax4.set_ylabel("Polarity Score", fontsize=12)
        ax4.set_xlabel("Sentiment Category", fontsize=12)
        ax4.grid(axis="y", alpha=0.3)
        ax4.axhline(y=0, color="red", linestyle="--", linewidth=1, alpha=0.5)
    plt.tight_layout()
    output_path = os.path.join(output_folder, "analysis_summary.png")
    plt.savefig(output_path, dpi=300, bbox_inches="tight")
    plt.close()
    print(f"   âœ“ ìš”ì•½ ì°¨íŠ¸ ì €ì¥: {output_path}")


# ============================================================================
# 6. ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„± (ì œí’ˆë³„ í‰ê·  í‰ì  ì™„ì „ ì œê±°)
# ============================================================================


def generate_analysis_report(
    df, keywords_by_sentiment, improvement_areas, output_folder
):
    print("\n[ì¶”ê°€] ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    report_path = os.path.join(output_folder, "analysis_report.txt")
    with open(report_path, "w", encoding="utf-8") as f:
        f.write("=" * 80 + "\n")
        f.write("ë‹¤ì´ì–´íŠ¸ ìœ ì‚°ê·  ë¦¬ë·° ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸\n")
        f.write("=" * 80 + "\n\n")
        # 1. ì „ì²´ í†µê³„
        f.write("[ 1. ì „ì²´ í†µê³„ ]\n")
        f.write("-" * 80 + "\n")
        total_reviews = len(df)
        avg_rating = df["rating"].mean()
        positive_ratio = (df["sentiment_category"] == "positive").sum() / total_reviews
        negative_ratio = (df["sentiment_category"] == "negative").sum() / total_reviews
        neutral_ratio = (df["sentiment_category"] == "neutral").sum() / total_reviews
        avg_polarity = df["sentiment_polarity"].mean()
        f.write(f"ì´ ë¦¬ë·° ìˆ˜: {total_reviews:,}ê°œ\n")
        f.write(f"í‰ê·  í‰ì : {avg_rating:.2f} / 5.0\n")
        f.write(f"ê¸ì • ë¦¬ë·° ë¹„ìœ¨: {positive_ratio:.1%}\n")
        f.write(f"ì¤‘ë¦½ ë¦¬ë·° ë¹„ìœ¨: {neutral_ratio:.1%}\n")
        f.write(f"ë¶€ì • ë¦¬ë·° ë¹„ìœ¨: {negative_ratio:.1%}\n")
        f.write(f"í‰ê·  ê°ì„± ê·¹ì„±: {avg_polarity:.3f} (-1 ~ 1)\n\n")
        # 2. ê°ì„±ë³„ ì£¼ìš” í‚¤ì›Œë“œ
        f.write("[ 2. ê°ì„±ë³„ ì£¼ìš” í‚¤ì›Œë“œ ]\n")
        f.write("-" * 80 + "\n")
        for sentiment in ["positive", "neutral", "negative"]:
            if sentiment in keywords_by_sentiment and keywords_by_sentiment[sentiment]:
                f.write(f"\nâ–  {sentiment.upper()} ë¦¬ë·° í‚¤ì›Œë“œ:\n")
                keywords = keywords_by_sentiment[sentiment][:15]
                for i, (word, score) in enumerate(keywords, 1):
                    f.write(f"   {i:2d}. {word:30s} (TF-IDF: {score:.4f})\n")
        f.write("\n")
        # 3. ê³ ê° ë¶ˆë§Œ ì˜ì—­ ë¶„ì„
        f.write("[ 3. ê³ ê° ë¶ˆë§Œ ì˜ì—­ ë¶„ì„ (ë¶€ì •/ì¤‘ë¦½ ë¦¬ë·° ê¸°ë°˜) ]\n")
        f.write("-" * 80 + "\n")
        if improvement_areas:
            f.write(f"ì´ {len(improvement_areas)}ê°œ ë¶ˆë§Œ ì˜ì—­ì´ í™•ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.\n\n")
            for i, area in enumerate(improvement_areas, 1):
                f.write(f"[{i}ìˆœìœ„] {area['description']}\n")
                f.write(f"  â”” ì–¸ê¸‰ íšŸìˆ˜: {area['mention_count']}íšŒ\n")
                f.write(f"  â”” ì˜ì—­ ì½”ë“œ: {area['area']}\n")
                if area["sample_reviews"]:
                    f.write(f"  â”” ê³ ê° ì˜ê²¬ ìƒ˜í”Œ:\n")
                    for j, sample in enumerate(area["sample_reviews"][:2], 1):
                        snippet = sample["review_snippet"][:100] + "..."
                        f.write(f"     {j}. (í‰ì  {sample['rating']}) {snippet}\n")
                f.write("\n")
        else:
            f.write("íŠ¹ë³„í•œ ë¶ˆë§Œ ì˜ì—­ì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n\n")
        f.write("\n" + "=" * 80 + "\n")
        f.write("ë¶„ì„ ì™„ë£Œ\n")
        f.write("=" * 80 + "\n")
    print(f"   âœ“ ë¦¬í¬íŠ¸ ì €ì¥: {report_path}")
    return report_path


# ============================================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ============================================================================


def main():
    """ë©”ì¸ ë¶„ì„ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    reviews_path = "Amazon_review/amazon_reviews.csv"
    products_path = "Amazon_review/amazon_products.csv"
    if not os.path.exists(reviews_path):
        print(f"âŒ ì˜¤ë¥˜: {reviews_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("   í¬ë¡¤ë§ ì½”ë“œë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”.")
        return
    if not os.path.exists(products_path):
        print(f"âŒ ì˜¤ë¥˜: {products_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    try:
        reviews_df, products_df = load_and_preprocess_data(reviews_path, products_path)
        reviews_df = perform_sentiment_analysis(reviews_df)
        keywords_by_sentiment = extract_keywords_by_sentiment(reviews_df, n_keywords=15)
        improvement_areas = extract_improvement_insights(reviews_df)
        create_visualizations(reviews_df, keywords_by_sentiment, improvement_areas)
        report_path = generate_analysis_report(
            reviews_df,
            keywords_by_sentiment,
            improvement_areas,
            "Amazon_review_analysis",
        )
        print("\n" + "=" * 80)
        print("âœ… ë¦¬ë·° ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print("=" * 80)
        print(f"\nğŸ“Š ìƒì„±ëœ íŒŒì¼:")
        print(f"   - ê¸ì • ë¦¬ë·° ë¶„ì„: Amazon_review_analysis/analysis_positive.png")
        print(f"   - ì¤‘ë¦½ ë¦¬ë·° ë¶„ì„: Amazon_review_analysis/analysis_neutral.png")
        print(f"   - ë¶€ì • ë¦¬ë·° ë¶„ì„: Amazon_review_analysis/analysis_negative.png")
        print(f"   - ì „ì²´ ìš”ì•½: Amazon_review_analysis/analysis_summary.png")
        print(f"   - ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸: {report_path}")
        print(f"\nğŸ“ˆ ë¶„ì„ ê²°ê³¼ ìš”ì•½:")
        print(f"   â€¢ ì´ ë¦¬ë·° ìˆ˜: {len(reviews_df):,}ê°œ")
        print(f"   â€¢ í‰ê·  í‰ì : {reviews_df['rating'].mean():.2f}/5.0")
        print(
            f"   â€¢ ê¸ì • ë¦¬ë·°: {(reviews_df['sentiment_category'] == 'positive').sum() / len(reviews_df):.1%}"
        )
        if improvement_areas:
            print(
                f"   â€¢ ì£¼ìš” ë¶ˆë§Œ ì˜ì—­ (1ìˆœìœ„): {improvement_areas[0]['description']} ({improvement_areas[0]['mention_count']}íšŒ)"
            )
            print(f"   â€¢ ë°œê²¬ëœ ë¶ˆë§Œ ì˜ì—­: {len(improvement_areas)}ê°œ")
        else:
            print(f"   â€¢ íŠ¹ë³„í•œ ë¶ˆë§Œ ì˜ì—­ ì—†ìŒ")
    except Exception as e:
        print(f"\nâŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
