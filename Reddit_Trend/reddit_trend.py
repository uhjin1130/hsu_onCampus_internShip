# -*- coding: utf-8 -*-
"""Reddit_trend.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11KN_q_0YwNBo10VgIMkFcE8eLiOVNexX
"""

!pip install asyncpraw
!pip install nest_asyncio
import asyncpraw
import asyncio
import nest_asyncio
import pandas as pd
from datetime import datetime

# 코랩 환경에서 asyncio.run() 중복 호출 문제 해결
nest_asyncio.apply()

# Reddit 게시물 수집 함수
async def get_reddit_posts(reddit, subreddit, query, keyword, trend_id_start=1, limit=20):
    """
    특정 서브레딧에서 query로 검색한 게시물을 가져와 제출 요구 포맷에 맞게 반환
    """
    posts = []
    subreddit_obj = await reddit.subreddit(subreddit)

    async for submission in subreddit_obj.search(query, limit=limit):
        # UTC timestamp → YYYY-MM-DD 변환
        date_str = datetime.utcfromtimestamp(submission.created_utc).strftime("%Y-%m-%d")

        posts.append({
            "trend_id": trend_id_start,
            "date": date_str,
            "platform": "reddit",
            "keyword": keyword,
            "post_text": submission.title + " | " + (submission.selftext[:200] if submission.selftext else ""),
            "hashtags": f"#probiotics #{keyword.replace(' ', '')} #{subreddit}"
        })
        trend_id_start += 1

    return posts, trend_id_start

async def main():
    """
    여러 키워드 × 여러 서브레딧에서 Reddit 게시물을 크롤링하고 CSV로 저장
    """
    print("Reddit 게시물 크롤링 시작\n")

    # Reddit API 인증
    async with asyncpraw.Reddit(
        client_id="bJ6zQg3D7kFgGELCBhzc5g",
        client_secret="tlnM1gxcz5aW_3RSnGpyT6zdpEop0A",
        user_agent="wevcrawlermini"
    ) as reddit:
        all_posts = []
        trend_id = 1

        # 검색 대상 서브레딧과 키워드 정의
        subreddits = ["loseit", "nutrition", "supplements", "guthealth"]
        queries = {
            "probiotics weight loss": "체중 감량",
            "probiotics taste": "맛 관련",
            "probiotics side effects": "부작용 관련"
        }

        # 각 서브레딧 × 각 키워드마다 20개씩
        for query, label in queries.items():
            for sub in subreddits:
                posts, trend_id = await get_reddit_posts(
                    reddit, sub, query, query, trend_id, limit=20
                )
                all_posts.extend(posts)
                print(f"[완료] {sub} - {label} ({len(posts)}개)")

        # DataFrame 변환
        df = pd.DataFrame(all_posts, columns=["trend_id","date","platform","keyword","post_text","hashtags"])

        # CSV 저장
        df.to_csv("reddit_sample.csv", index=False, encoding="utf-8-sig")
        print("\nCSV 저장 완료: reddit_sample.csv")
        print(f"총 저장된 행 수: {len(df)}")

# 실행
if __name__ == "__main__":
    asyncio.run(main())